{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc522881",
   "metadata": {},
   "source": [
    "# Implementing Recommendation Models using Surprise Library\n",
    "This notebook demonstrates loading MovieLens, training SVD, KNNBasic and NMF models using Surprise, evaluating them, performing grid-search, and generating Topâ€‘N recommendations.\n",
    "\n",
    "**Dataset:** MovieLens 100K (Surprise built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (run once if needed)\n",
    "# !pip install scikit-surprise pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07807dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surprise import Dataset, Reader, SVD, NMF, KNNBasic\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise import accuracy\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9b0ec",
   "metadata": {},
   "source": [
    "## 1) Load data and prepare train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c79654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MovieLens 100K\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print('Trainset users:', trainset.n_users, 'items:', trainset.n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e24365",
   "metadata": {},
   "source": [
    "## 2) Train baseline models: SVD, NMF, KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbcd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "svd = SVD(n_factors=50, random_state=42)\n",
    "svd.fit(trainset)\n",
    "pred_svd = svd.test(testset)\n",
    "rmse_svd = accuracy.rmse(pred_svd, verbose=True)\n",
    "mae_svd = accuracy.mae(pred_svd, verbose=True)\n",
    "\n",
    "# NMF\n",
    "nmf = NMF(n_factors=15, random_state=42)\n",
    "nmf.fit(trainset)\n",
    "pred_nmf = nmf.test(testset)\n",
    "rmse_nmf = accuracy.rmse(pred_nmf, verbose=True)\n",
    "mae_nmf = accuracy.mae(pred_nmf, verbose=True)\n",
    "\n",
    "# KNNBasic (item-based)\n",
    "sim_options = {'name': 'cosine', 'user_based': False}\n",
    "knn = KNNBasic(k=40, sim_options=sim_options)\n",
    "knn.fit(trainset)\n",
    "pred_knn = knn.test(testset)\n",
    "rmse_knn = accuracy.rmse(pred_knn, verbose=True)\n",
    "mae_knn = accuracy.mae(pred_knn, verbose=True)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['SVD', 'NMF', 'KNNBasic'],\n",
    "    'RMSE': [rmse_svd, rmse_nmf, rmse_knn],\n",
    "    'MAE': [mae_svd, mae_nmf, mae_knn]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63618a56",
   "metadata": {},
   "source": [
    "## 3) Cross-validation (5-fold) comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84882450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate SVD and NMF for more robust comparison\n",
    "models = {'SVD': SVD(n_factors=50, random_state=42),\n",
    "          'NMF': NMF(n_factors=15, random_state=42)}\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    cv = cross_validate(model, data, measures=['RMSE','MAE'], cv=5, verbose=False)\n",
    "    cv_results[name] = {'RMSE_mean': np.mean(cv['test_rmse']), 'MAE_mean': np.mean(cv['test_mae'])}\n",
    "\n",
    "pd.DataFrame(cv_results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710cf01f",
   "metadata": {},
   "source": [
    "## 4) Grid Search for SVD hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for SVD\n",
    "param_grid = {'n_factors': [20,50,100], 'lr_all': [0.005, 0.01], 'reg_all': [0.02, 0.1]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "print('Best RMSE score:', gs.best_score['rmse'])\n",
    "print('Best params:', gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2de754",
   "metadata": {},
   "source": [
    "## 5) Generate Top-N recommendations for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to get Top-N recommendations from predictions\n",
    "def get_top_n(predictions, n=10, min_rating=4.0):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = [(iid, est) for (iid, est) in user_ratings if est >= min_rating][:n]\n",
    "    return top_n\n",
    "\n",
    "# Predict on all pairs (build anti-testset)\n",
    "trainset_full = data.build_full_trainset()\n",
    "algo = SVD(n_factors=gs.best_params['rmse']['n_factors'] if hasattr(gs, 'best_params') else 50)\n",
    "algo.fit(trainset_full)\n",
    "anti_testset = trainset_full.build_anti_testset()\n",
    "predictions_all = algo.test(anti_testset)\n",
    "top_n = get_top_n(predictions_all, n=10, min_rating=4.0)\n",
    "\n",
    "# Show Top-10 for a random sample user\n",
    "sample_user = list(top_n.keys())[0]\n",
    "sample_user, top_n[sample_user]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46cbae",
   "metadata": {},
   "source": [
    "## 6) Precision@K and Recall@K evaluation for Top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test set ground truth from original testset: items with rating >=4 are relevant\n",
    "test_df = pd.DataFrame(testset, columns=['user','item','rating','_'])\n",
    "test_relevant = test_df[test_df['rating']>=4.0].groupby('user')['item'].apply(set).to_dict()\n",
    "\n",
    "def precision_recall_at_k(top_n, test_relevant, k=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for uid, recs in top_n.items():\n",
    "        if uid not in test_relevant or len(test_relevant[uid])==0:\n",
    "            continue\n",
    "        recommended = [iid for (iid, _) in recs][:k]\n",
    "        relevant = test_relevant[uid]\n",
    "        hits = len([iid for iid in recommended if iid in relevant])\n",
    "        precisions.append(hits / k)\n",
    "        recalls.append(hits / len(relevant))\n",
    "    return np.mean(precisions) if precisions else None, np.mean(recalls) if recalls else None\n",
    "\n",
    "prec, rec = precision_recall_at_k(top_n, test_relevant, k=10)\n",
    "prec, rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351a662",
   "metadata": {},
   "source": [
    "## 7) Plot RMSE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e56f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE comparison (from previous results DataFrame)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(results['Model'], results['RMSE'])\n",
    "plt.title('RMSE Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db2cac",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Use cross-validation and hold-out test sets for robust evaluation.\n",
    "- Grid search can be extended for other algorithms (NMF, KNN).\n",
    "- Consider scaling for large datasets (sample, incremental training, or matrix factorization libraries)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}